<!DOCTYPE html>
<html>
<head>
    <title>Projects</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="https://RyanGale-AK.github.io/assets/css/main.css"/>
    <noscript><link rel="stylesheet" href="https://RyanGale-AK.github.io/assets/css/noscript.css"/></noscript>
    <style>
        :root{--site-background : url("https://RyanGale-AK.github.io/images/alaska_range.jpg")};
    </style> 
</head>

<body class="is-preload">

    
        <div id="wrapper">

            
                <header id="header">
                    <div class="logo">
                        <a href="/"><img src="../images/home_icon.png" style="width:50%;height:50%;margin-top: 20%;"></a>
                    </div>
                    <div class="content">
                        <div class="inner">
                            <h1>Projects</h1>
                            
                        </div>
                    </div>
                    <nav>
                        <ul>                  
                            <li>
                                <a href="#deep-q-learning">Playing Atari Pong with a Deep Q-Network</a>
                            </li>
                        </ul>
                        <ul>
                            <li>
                                <a href="#transformers">Transformers Are All You Need</a>
                            </li>
                        </ul>
                    </nav>
                </header>
            
                <div id="main">

                    
                        
                            
                                <article id="deep-q-learning">
                                    <h1 class="major">Playing Atari Pong <br/> 
                                        with a Deep <br/> Q-Network</h1>
                                    <!-- <span class="image main"><img src="" alt="" style="width:500px;height:400px;"/></span> -->
<p>This article chronicles my dive into reinforcement learning and the
training of a deep neural network agent to play Pong on the Atari 2600.
This compiles information from a handful of fantastic teachers including
Lilian Weng, Richard Sutton, Andrej Karpathy, Brandon Kates, Ryan Benmalek, among
others listed in the references.</p>

<h2 id="background-background-unnumbered-unnumbered">Background </h2>

<p>Reinforcement learning is a subset of machine learning involving the
training of an agent to optimally interact with its environment. As the
agent explores its environment it will receive rewards that indicate how
well it is doing at its task. The goal of reinforcement learning is to
create agents that can maximize accumulative reward in complex,
large-scale environments. This often takes the form of a robot
interacting with its environment or a video game bot. The video game
environment has been a popular modality because it offers well defined
rewards and is itself simulated. For my delve into reinforcement
learning (RL), I chose to create a bot that can play Atari Pong. This is
in the spirit of the first deep learning RL agent created by Volodymyr
Minh in 2013. After implementing a Deep Q-Network (DQN)
architecture, I explore more recent improvements suggested in the
literature and implement distributed training.<br>
Most RL problems can be formulated as a Markov Decision Process (MDP).
An MDP is a directed graph that contains possible states in an
environment with each edge corresponding to actions that an agent can
take. Each action has an associated probability of being taken in
relation to the expected reward that is estimated for that action.</p>

<p>
    <figure id="mdp" class="unnumbered">
        <center>
            Example of a Markov decision process
            <img src="images/mdp.jpg" alt="Example of a Markov decision process." style="width:500px;height:300px;">
        </center>
    </figure>
</p>

<p>In the case of Pong, the agent has no information about the game engine,
instead we simply feed it a vector of raw pixel values representing the
current screen. For any given game, the sequence of observation-action
pairs that led to the current screen image encodes a distinct state. Now
as the game progresses the sequence of moves becomes very large,
tracking the entire history is computationally infeasible. That's where
the Markov property comes in to save the day: the future only depends on
the current state. Since the current state contains all of the
statistics necessary for making future decisions, the agent only needs
to know the location of the two paddles and ball.</p>

<p>
    <figure id="agent_environment" class="unnumbered">
    <center>
        RL models rely on the Markov property
    </center>
    <center>
        <img src="images/agent_environment.png" 
        alt="RL models rely on the Markov property." style="width:350px;height:150px;">
        </center>
    </figure>
</p>

<p>Now the agent can make an optimal decision according to some rule or
strategy it develops as it makes a series of good or bad moves. In RL,
this strategy is called a policy, <span  class="math">\(\pi\)</span>. The policy maps an agent's
state, <span  class="math">\(s\)</span>, to an action, <span  class="math">\(a\)</span> and can be deterministic or stochastic.
The agent's goal, of course, is having the optimal policy, but it begins
knowing nothing about how to play Pong. Given an image, the agent must
choose to move up, down, or stay still and every transition has a reward
of <span  class="math">\(0\)</span> except for the end game state where the reward is <span  class="math">\(\pm 1\)</span>
depending on whether the agent won the round (although in a game both
players compete to 21 points, the agents reward is reset after each
round). But if the agent begins with no knowledge of playing, how does
it develop a good policy?</p>

<p>A <strong>value function</strong> does exactly what it sounds like, it evaluates how
good a given state or observation-action pair is based on the future
expected reward. In Pong the only nonzero reward is at the end of a
round so the agent's current actions may not reap reward until a
thousand or so frames in the future. To combat this sparsity, we assign
discounted rewards based on how close an action is to the receipt of a
reward. If the paddle agent misses the ball, the most recent action will
be assigned a reward closer to <span  class="math">\(-1\)</span> than moves at the beginning of the
game (which are discounted with an exponentially decaying factor). More
formally, the value, under policy <span  class="math">\(\pi\)</span>, is the expected return of being
in state <span  class="math">\(s\)</span>: <span  class="math">\(V_{\pi}(s)=E_{\pi}[G_t|S_t=s]\)</span> Where <span  class="math">\(\boldsymbol G_t\)</span>,
the <strong>return</strong>, is the sum of discounted rewards gained by future
actions (rewards are allocated upon completion of a game):</p>

<center>
<span  class="math">\(G_t=R_{t+1}+\gamma R_{t+2}+\dots=\sum\limits_{k=0}^{\infty}\gamma^kR_{t+k+1}\)</span>
</center>
<p>The discounting factor <span  class="math">\(\gamma\in[0,1]\)</span> penalizes future rewards.</p>

<p>
<figure id="background" class="unnumbered">
<center>
    The most recent action prior to a reward is the most relevant
    <img src="images/discounted.png" alt="The most recent action prior to a reward is the most relevant.
[@discount]" style="width:400px;height:250px;">
</center></figure></p>

<h2 id="qlearning-qlearning-unnumbered-unnumbered" width="75mm">Q-Learning </h2>

<p>Q-learning is a classic algorithm that broke out in the early days of RL
in 1992. It applies an <strong>action-value</strong> function based on the quality
<span  id="qlearning-qlearning-unnumbered-unnumbered" width="75mm" class="math">\(\boldsymbol Q_{\pi}\)</span> of an observation-action pair:</p>

<p><span  id="q-learning" class="unnumbered" class="math">\[Q_{\pi}(s,a)=E_{\pi}[G_t|S_t=s,A_t=a]\]</span></p>

<p>The algorithm is as follows:</p>

<ol>
<li><p>Beginning in state <span  class="math">\(S_t\)</span> pick action <span  class="math">\(A_t=argmax_{a\in A}Q(S_t,a)\)</span>
using <span  class="math">\(\epsilon\)</span>-greedy.</p>

<p><span  class="math">\(\epsilon\)</span>-greedy is a strategy that chooses to take action
<span  class="math">\(A_t=argmax_{a\in A}Q(S_t,a)\)</span> with probability <span  class="math">\(\epsilon\)</span>, otherwise
choose a move at random. When the agent first begins it knows
absolutely nothing, it is given an image and must randomly explore
the game states in order to get enough experience to know which
moves are good. This is known as exploration. Once enough moves are
understood as good and bad, choosing <span  class="math">\(A_t=argmax_{a\in A}Q(S_t,a)\)</span>
allows the model to leverage known moves, this is known as
exploitation. In the beginning of learning we want the agent to
explore moves randomly and increase the accuracy of action-value
estimates. As more experience is gained we want to exploit moves we
know to produce high reward. However, just because an agent knows a
few good moves does not mean there are not better moves out there, a
little randomness will nudge an agent away from sub-par behavior.
<span  class="math">\(\epsilon\)</span>-greedy then strikes a balance between exploration and
exploitation.</p></li>

<li><p>Observe reward <span  class="math">\(R_{t+1}\)</span> and achieve state <span  class="math">\(S_{t+1}\)</span></p></li>

<li><p>Update:
<span  class="math">\(Q(S_t,A_t)=Q(S_t,A_t)+\alpha (R_{t+1}+\gamma max_{a\in A}Q(S_{t+1},a)-Q(S_t,A_t))\)</span></p>

<p>We make an estimate of the action-value of the next move
<span  class="math">\(Q*=max_{a\in A}Q(S_{t+1},a)\)</span> in order to estimate the expected
return of <span  class="math">\(A_t\)</span>. In theory, we can think of <span  class="math">\(Q*\)</span> as a giant table
containing all observation-action pairs that are possible in a game
of pong. As the agent plays dozens and then hundreds of games, this
table is slowly filled until finally the best move is known for any
possible state. Unfortunately this rote memorization, while
theoretically possible, is not computationally feasible for an
environment with a massive state-space. This is where a deep neural
net comes in.</p></li>

<li><p><span  class="math">\(t=t+1\)</span> and repeat until optimal action-value estimates are
discovered</p></li>
</ol>

<h2 id="deep-qnetwork-deepqnetwork-unnumbered-unnumbered">Deep Q-Network </h2>

<p>Twenty-one years after the breakthrough of Q-learning, Google DeepMind
presented the first deep neural network using reinforcement learning,
catching the media's attention for being able to beat human performance
on multiple Atari games<a class="cite" href="#original"></a>. Because 'memorizing' <span  class="math">\(Q*\)</span> is
infeasible when the state-space is very large, this adaptation uses a
neural network to approximate it. If we use the neural net parameters
<span  class="math">\(\theta\)</span>, our new action-value function becomes <span  class="math">\(Q(s,a;\theta)\)</span></p>

<p>Here is an implementation of the DQN with a few improvements:</p>
<pre><code>  def run(self, num_episodes):
        # Load policy weights into target network
        self.q_target.load_state_dict(self.q.state_dict())  
        self.optimizer = optim.Adam(self.q.parameters(), lr=self.learning_rate)

        if self.saved_model:
            # Load pretrained model
            self.q.load_state_dict(torch.load(saved_model)) 

        self.beginLogging()
        env = self.env
        best_episode_score = float(&#39;-Inf&#39;)
        score = 0.0
        total_frames = 0

        # Start first game
        state = get_state(env.reset()) 
        for episode in tqdm(
                    range(self.start_episode,self.start_episode +
                    num_episodes)):

            # anneal 100% to 1% over training (exploration vs exploitation)
            epsilon = self.epsilon_decay(total_frames)
            episode_score = 0
            done = False
            while not done:
                action = self.q.sample_action(
                    torch.Tensor(state).unsqueeze(0).to(device), epsilon)

                obs, reward, done, info = env.step(action)

                next_state = get_state(obs)

                done_mask = 0.0 if done else 1.0
                self.memory.put((state,action,reward,next_state,done_mask))
                state = next_state

                score += reward
                episode_score += reward

                if total_frames &gt; self.training_frame_start:
                    self.train()

                # Copy policy weights to target
                if total_frames%self.update_target_interval == 0:
                    self.q_target.load_state_dict(self.q.state_dict())
                # Save policy weights
                if total_frames%self.save_interval==0:
                    torch.save(self.q.state_dict(),
                    os.path.join(self.save_location, &#39;policy_%s.pt&#39; % episode))
                # Reset environment for the next game
                if done:
                    state = get_state(env.reset())
                total_frames += 1

            best_episode_score = max(best_episode_score, episode_score)
            # Print updates every episode
            out = &#34;n_episode : {}, Total Frames : {}, Average Score : {:.1f},
            Episode Score : {:.1f}, Best Score : {:.1f}, n_buffer : {}, eps :
            {:.1f}%&#34;.format(
                episode, total_frames, score/episode, episode_score,
                best_episode_score, len(self.memory), epsilon*100)
            print(out)
            self.log(out)

        # save final model weights
        torch.save(self.q.state_dict(), 
                    os.path.join(self.save_location, 
                    &#39;policy_final.pt&#39;))</code></pre>
<p>Full source code can be found <a href="https://github.com/RyanGale-AK/DQN">here</a></p>
<p>
This implementation utilizes an experience replay buffer, a memory bank
of previous observation-action pairs and their associated rewards. By
updating the network from this buffer, we can learn from previous
actions multiple times (providing stronger assurance of correlation
between a specific observation-action and the reward) and make our data
more independent and identically distributed (a property that provides
stronger convergence guarantees for our function approximator <span  id="deep-q-network" class="unnumbered" class="math">\(Q*\)</span>). The
buffer stores the episode (each round of game play) step
<span  id="deep-q-network" class="unnumbered" class="math">\(e = (state,action,reward,next_state,done_mask)\)</span>. During the update
step, samples are randomly drawn from the replay memory which improves
data efficiency and decorrelates experiences that occur close together
in time. After 10,000 episodes and roughly 500,000 frames of experience
this agent was able to achieve impressive results, winning almost every
match-up 21-0.</p>

<p>
    <figure id="agent_environment" class="unnumbered">
    <center>
        Gameplay
    </center>
    <center>
    <video width="300" height="400" controls>
        <source src="videos/pong.mp4" type="video/mp4">
    </video>
    </center>
    </figure>
</p>

<h2 id="dqn-variants-dqnvariants-unnumbered-unnumbered" source="video/pong.mp4" width="125mm">DQN Variants </h2>

<p>The standard DQN architecture can inflate the value of an
observation-action pair making it difficult to learn better maneuvers
later in training. In recent years there have been a handful of proposed
improvements to the algorithm, I chose to explore double DQN and
prioritized replay buffer.</p>

<p>In Double DQN (DDQN) we decouple the Q-value estimation with a second
neural net allowing the estimates to be regularized<a class="cite" href="#ddqn"></a>. More
specifically, one network, <span  id="dqn-variants-dqnvariants-unnumbered-unnumbered" source="video/pong.mp4" width="125mm" class="math">\(Q_{\theta}\)</span>, chooses the action while a
second, <span  id="dqn-variants-dqnvariants-unnumbered-unnumbered" source="video/pong.mp4" width="125mm" class="math">\(Q_{\theta^-}\)</span> evaluates that action. The new update step is:
<center>
<span  id="dqn-variants-dqnvariants-unnumbered-unnumbered" source="video/pong.mp4" width="125mm" class="math">\(Q(S_t,a)=R_{t+1}+\gamma Q(S_{t+1}, max_{a\in A}Q(S_{t+1},a;\theta_t);\theta_t^-)\)</span></p>
</center>


<p>The authors of DDQN show that this decoupling effectively reduces the
tendency of the DQN to overestimate observation-action pairs in
large-scale problems.</p>

<p>A second interesting adaptation is updating the Q network with
experiences that have high importance. Instead of drawing randomly from
the replay buffer, with a prioritized replay buffer we choose the
experience that has the biggest shift in Q-value between the current and
next state (known as the temporal difference error). The authors note
that this allows the agent to &quot;replay important transitions more
frequently, and therefore learn more efficiently&quot; <a class="cite" href="#prioritized"></a>.</p>

<p>
<figure id="dqn-variants" class="unnumbered">
<center>
Experimenting with DQN variants
</center>
<center>
<img src="images/model_comparison.png" alt="Experimenting with DQN
variants." style="width:500px;height:300px;">
</center>
</figure></p>

<p>In my experiments, I've only noticed a very slight increase in
performance with the DDQN. Because the game is not sufficiently complex,
DQN appears to not drastically overestimate the value of certain moves
(in a game like PacMan DDQN will likely show a performance boost).
Similarly, prioritized replay buffer with DDQN achieved similar
performance with a longer training period.</p>

<p>I found that training the DQN to play Ms Pacman and Space Invaders took
a significant number of rounds and was not able to successfully beat the
game. It appears that the algorithm has low sample efficiency when the
Q-function it needs to approximate is very complex (i.e. Pacman). One
solution could be to increase the model capacity to allow for a more
complex Q-function approximation but this will not scale well. Although
initial attempts with DQN variants didn't show promising results,
meta-learning offers a path forward. In future work I'm interested in
applying Hebbian learning rules to develop a more adaptable DQN. In
their recent paper, &quot;Meta-Learning through Hebbian Plasticity in Random
Networks,&quot; Elias Najarro and Sebastian Risi from the University of
Copenhagen showed how discovered Hebbian rules enable an agent to
navigate a dynamical 2D-pixel environment<a class="cite" href="#hebbian"></a>. These network rules
allow the model to continually self-organize its weights which allow it
to adapt to starkly different learning environments. Stay tuned for
updates on my Github RyanGale-AK.</p>

<h2 id="conclusion-conclusion-unnumbered-unnumbered" width="125mm">Conclusion </h2>

<p>DQN is an impressively adaptive RL algorithm that models some aspects of
how we learn to play games ourselves; first exploring new moves with the
risk of defeat but the reward of knowledge, next exploiting known moves
enticed by their anticipated reward. I was initially intrigued by the
algorithm's impressive ability to out-perform a human player on 3 of the
57 original Atari games (Pong, Breakout, and Enduro <a class="cite" href="#original"></a>). Even
more exciting, while I was working on this project DeepMind published
Agent57, the first algorithm to achieve above the human baseline on all
57 games<a class="cite" href="#agent57"></a>! This monumental achievement celebrates 7 years of
continuous research attempting to surpass the Atari benchmark, and
ushers in an exciting new chapter of RL where the field extends beyond
video games and into real-world applications. Along this same thread,
Google research has recently proposed the Real-World Reinforcement
Learning Challenge Framework<a class="cite" href="#real"></a>. This provides researchers with a
new benchmark that simulates the challenges of real-world environments,
including dimensions such as safety, delays, and perturbations. While we
all have been adapting to these environmental challenges since day 1,
robots still struggle. It is our duty to ensure these new friends of
ours are trained and deployed with the highest regard to human safety
and impartiality toward gender, race, creed, wealth, and all other
facets of the human dimension.</p>

<h2 id="Sources-unnumbered-unnumbered" width="125mm">References</h2>

<p>
    [1]    Adri`a Puigdom`enech et al. Agent57: Outperforming the human Atari benchmark. <a href="https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark">link</a>
</p><p>    
    [2]    Daniel Mankowitz et al. Real-World Reinforcement Learning (RWRL) Challenge Framework. <a href="https://github.com/google-research/realworldrl_suite">link</a>
</p><p>
    [3]    Elias Najarro et al. Meta-Learning through Hebbian Plasticity in Random Networks. <a href="https://arxiv.org/abs/2007.02686">link</a>
</p><p>    
    [4]    Mnih Volodymyr et al. Playing atari with deep reinforcement learning. <a href="https://arxiv.org/abs/1312.5602">link</a>
</p><p>
    [5]    Tom  Schaul  et  al. Prioritized Experience Replay. <a href="https://arxiv.org/abs/1511.05952">link</a>
</p><p>
    [6]    Jake Bennett. The Algorithm Behind the Curtain: Reinforcement Learning Concepts. <a href="https://randomant.net/reinforcement-learning-concepts/">link</a>
</p><p>
    [7]    Hado  van  Hasselt  et  al. Deep Reinforcement Learning with Double Q-learning. <a href="https://arxiv.org/abs/1509.06461">link</a>
</p><p>
    [8]    Andrej Karpathy. Deep RL Bootcamp Lecture 4B Policy Gradients. <a href="https://www.youtube.com/watch?v=tqrcjHuNdmQ">link</a>
</p><p>
    [9]    R.S.  Sutton  and  A.G.  Barto. Reinforcement Learning: An Introduction. <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">link</a>
</p>

                                </article>
                            

                                <article id="transformers">
                                    <h1 class="major">Transformers and Time-Series Forecasting: Random Biased Walk</h1>
                                    <!-- <span class="image main"><img src="" alt="" style="width:500px;height:400px;"/></span> -->
<p>This article is under construction.  See my <a href="https://github.com/RyanGale-AK">github</a> for the most recent updates.</p>

<!-- <h2 id="background-background-unnumbered-unnumbered">Background </h2> -->

<!-- <h2 id="Sources-unnumbered-unnumbered" width="125mm">References</h2>

<p>
    [1]    Adri`a Puigdom`enech et al. Agent57: Outperforming the human Atari benchmark. <a href="https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark">link</a>
</p> -->


                                </article>
                        
                    

                </div>

                
<footer id="footer">
    <pre class="copyright">Mount Huntington, Alaska Range: Photo by Ryan Gale</pre>
</footer>

        </div>

    
    <div id="bg"></div>

</body>



    
    <script src=https://RyanGale-AK.github.io/assets/js/jquery.min.js></script>
    <script src=https://RyanGale-AK.github.io/assets/js/browser.min.js></script>
    <script src=https://RyanGale-AK.github.io/assets/js/breakpoints.min.js></script>
    <script src=https://RyanGale-AK.github.io/assets/js/util.js></script>
    <script src=https://RyanGale-AK.github.io/assets/js/main.js></script>
    <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


</html>

